The simplest kind of feedforward neural network (FNN) is a linear network, which consists of a single layer of output nodes; the inputs are fed directly to the outputs via a series of weights. The sum of the products of the weights and the inputs is calculated at each node. The mean squared errors between these calculated outputs and the given target values are minimized by creating an adjustment to the weights. This technique has been known for over two centuries as the method of least squares or linear regression. It was used as a means of finding a good rough linear fit to a set of points by Legendre (1805) and Gauss (1795) for the prediction of planetary movement.[7][8][9][10][11]

Warren McCulloch and Walter Pitts[12] (1943) also considered a non-learning computational model for neural networks.[13]

In the late 1940s, D. O. Hebb[14] created a learning hypothesis based on the mechanism of neural plasticity that became known as Hebbian learning. Hebbian learning is considered to be a 'typical' unsupervised learning rule and its later variants were early models for long term potentiation. These ideas started being applied to computational models in 1948 with Turing's "unorganized machines". Farley and Wesley A. Clark[15] were the first to simulate a Hebbian network in 1954 at MIT. They used computational machines, then called "calculators". Other neural network computational machines were created by Rochester, Holland, Habit, and Duda[16] in 1956. In 1958, psychologist Frank Rosenblatt invented the perceptron, the first implemented artificial neural network,[17][18][19][20] funded by the United States Office of Naval Research.[21]