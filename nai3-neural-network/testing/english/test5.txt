During the AI Winter era, however, research outside the United States continued, especially in Eastern Europe. By the time Minsky and Papert's book on Perceptrons came out, methods for training multilayer perceptrons (MLPs) were already known. The first deep learning MLP was published by Alexey Grigorevich Ivakhnenko and Valentin Lapa in 1965, as the Group Method of Data Handling.[26][27][28] The first deep learning MLP trained by stochastic gradient descent[29] was published in 1967 by Shun'ichi Amari.[30][31] In computer experiments conducted by Amari's student Saito, a five layer MLP with two modifiable layers learned useful internal representations to classify non-linearily separable pattern classes.[31]

Self-organizing maps (SOMs) were described by Teuvo Kohonen in 1982.[32][33] SOMs are neurophysiologically inspired[34] neural networks that learn low-dimensional representations of high-dimensional data while preserving the topological structure of the data. They are trained using competitive learning.[32]

The convolutional neural network (CNN) architecture with convolutional layers and downsampling layers was introduced by Kunihiko Fukushima in 1980.[35] He called it the neocognitron. In 1969, he also introduced the ReLU (rectified linear unit) activation function.[36][10] The rectifier has become the most popular activation function for CNNs and deep neural networks in general.[37] CNNs have become an essential tool for computer vision.

A key in later advances in artificial neural network research was the backpropagation algorithm, an efficient application of the Leibniz chain rule (1673)[38] to networks of differentiable nodes.[10] It is also known as the reverse mode of automatic differentiation or reverse accumulation, due to Seppo Linnainmaa (1970).[39][40][41][42][10] The term "back-propagating errors" was introduced in 1962 by Frank Rosenblatt,[43][10] but he did not have an implementation of this procedure, although Henry J. Kelley[44] and Bryson[45] had dynamic programming based continuous precursors of backpropagation[26][46][47][48] already in 1960â€“61 in the context of control theory.[10] In 1973, Dreyfus used backpropagation to adapt parameters of controllers in proportion to error gradients.[49] In 1982, Paul Werbos applied backpropagation to MLPs in the way that has become standard.[50][46] In 1986 Rumelhart, Hinton and Williams showed that backpropagation learned interesting internal representations of words as feature vectors when trained to predict the next word in a sequence.[51]